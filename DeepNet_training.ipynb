{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "DATA_NUM_CLASSES        = 10\n",
    "DATA_CHANNELS           = 3\n",
    "DATA_ROWS               = 32\n",
    "DATA_COLS               = 32\n",
    "DATA_CROP_ROWS          = 28\n",
    "DATA_CROP_COLS          = 28\n",
    "DATA_MEAN               = np.array([[[125.30691805, 122.95039414, 113.86538318]]]) # CIFAR10\n",
    "DATA_STD_DEV            = np.array([[[ 62.99321928,  62.08870764,  66.70489964]]]) # CIFAR10\n",
    "NUM_PARALLEL_CALLS      = 4\n",
    "TRAINING_BATCH_SIZE      = 32\n",
    "SHUFFLE_BUFFER  = 5000\n",
    "EPOCHS = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_train(example):\n",
    "    # extract image and label from example\n",
    "    image = example[\"image\"]\n",
    "    label = example[\"label\"]\n",
    "  \n",
    "    # image is cast to float32, normalized, augmented and random cropped\n",
    "    # label is cast to int32\n",
    "    image = tf.math.divide(tf.math.subtract(tf.dtypes.cast(image, tf.float32), DATA_MEAN), DATA_STD_DEV)\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_crop(image, size=[DATA_CROP_ROWS, DATA_CROP_COLS, 3])\n",
    "    label = tf.dtypes.cast(label, tf.int32)\n",
    "    \n",
    "    # return image and label\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre processing for testing data\n",
    "def pre_processing_test(example):\n",
    "\n",
    "    # extract image and label from example\n",
    "    image = example[\"image\"]\n",
    "    label = example[\"label\"]\n",
    "\n",
    "    # image is cast to float32, normalized, augmented and center cropped\n",
    "    # label is cast to int32\n",
    "    image = tf.math.divide(tf.math.subtract(tf.dtypes.cast(image, tf.float32), DATA_MEAN), DATA_STD_DEV)\n",
    "    image = tf.image.crop_to_bounding_box(image, (DATA_ROWS - DATA_CROP_ROWS) // 2, (DATA_COLS - DATA_CROP_COLS) // 2, DATA_CROP_ROWS, DATA_CROP_COLS)\n",
    "    label = tf.dtypes.cast(label, tf.int32)\n",
    "    \n",
    "    # return image and label\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train, info = tfds.load(\"cifar10\", split=tfds.Split.TRAIN, with_info=True)\n",
    "dataset_test,  info = tfds.load(\"cifar10\", split=tfds.Split.TEST,  with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(history):\n",
    "\n",
    "    # training and validation data accuracy\n",
    "    acc     = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "\n",
    "    # training and validation data loss\n",
    "    loss     = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    # plot accuracy\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(acc, label='Training Accuracy')\n",
    "    plt.plot(val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([min(plt.ylim()), 1])\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    # plot loss\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylabel('Cross Entropy')\n",
    "    plt.ylim([0, 2.0])\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform training dataset\n",
    "dataset_train = dataset_train.map(pre_processing_train, num_parallel_calls=NUM_PARALLEL_CALLS)\n",
    "dataset_train = dataset_train.shuffle(buffer_size=SHUFFLE_BUFFER)\n",
    "dataset_train = dataset_train.batch(TRAINING_BATCH_SIZE)\n",
    "dataset_train = dataset_train.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# transform testing dataset\n",
    "dataset_test = dataset_test.map(pre_processing_test, num_parallel_calls=NUM_PARALLEL_CALLS)\n",
    "dataset_test = dataset_test.batch(TRAINING_BATCH_SIZE)\n",
    "dataset_test = dataset_test.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionResNet(object):\n",
    "    def __init__(self, input_shape=(32,32,3), num_classes=10):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def conv2d_bn(x,\n",
    "              filters,\n",
    "              kernel_size,\n",
    "              strides=1,\n",
    "              padding='same',\n",
    "              activation='relu',\n",
    "              use_bias=False,\n",
    "              name=None):\n",
    "        \"\"\"Utility function to apply conv + BN.\n",
    "        # Arguments\n",
    "            x: input tensor.\n",
    "            filters: filters in `Conv2D`.\n",
    "            kernel_size: kernel size as in `Conv2D`.\n",
    "            strides: strides in `Conv2D`.\n",
    "            padding: padding mode in `Conv2D`.\n",
    "            activation: activation in `Conv2D`.\n",
    "            use_bias: whether to use a bias in `Conv2D`.\n",
    "            name: name of the ops; will become `name + '_ac'` for the activation\n",
    "                and `name + '_bn'` for the batch norm layer.\n",
    "        # Returns\n",
    "            Output tensor after applying `Conv2D` and `BatchNormalization`.\n",
    "        \"\"\"\n",
    "        bn_name = None if name is None else name + '_bn'\n",
    "        x = tf.keras.layers.Conv2D(filters,\n",
    "                          kernel_size,\n",
    "                          strides=strides,\n",
    "                          padding=padding,\n",
    "                          name=name)(x)\n",
    "        x = tf.keras.layers.BatchNormalization(scale=False, name=bn_name)(x)\n",
    "        if activation is not None:\n",
    "            ac_name = None if name is None else name + '_ac'\n",
    "            x = tf.keras.layers.Activation(activation, name=ac_name)(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def inception_resnet_block(x, scale, block_type, block_idx, activation='relu'):\n",
    "        \"\"\"Adds a Inception-ResNet block.\n",
    "        This function builds 3 types of Inception-ResNet blocks mentioned\n",
    "        in the paper, controlled by the `block_type` argument (which is the\n",
    "        block name used in the official TF-slim implementation):\n",
    "            - Inception-ResNet-A: `block_type='block35'`\n",
    "            - Inception-ResNet-B: `block_type='block17'`\n",
    "            - Inception-ResNet-C: `block_type='block8'`\n",
    "        # Arguments\n",
    "            x: input tensor.\n",
    "            scale: scaling factor to scale the residuals (i.e., the output of\n",
    "                passing `x` through an inception module) before adding them\n",
    "                to the shortcut branch.\n",
    "                Let `r` be the output from the residual branch,\n",
    "                the output of this block will be `x + scale * r`.\n",
    "            block_type: `'block35'`, `'block17'` or `'block8'`, determines\n",
    "                the network structure in the residual branch.\n",
    "            block_idx: an `int` used for generating layer names.\n",
    "                The Inception-ResNet blocks\n",
    "                are repeated many times in this network.\n",
    "                We use `block_idx` to identify\n",
    "                each of the repetitions. For example,\n",
    "                the first Inception-ResNet-A block\n",
    "                will have `block_type='block35', block_idx=0`,\n",
    "                and the layer names will have\n",
    "                a common prefix `'block35_0'`.\n",
    "            activation: activation function to use at the end of the block\n",
    "                (see [activations](../activations.md)).\n",
    "                When `activation=None`, no activation is applied\n",
    "                (i.e., \"linear\" activation: `a(x) = x`).\n",
    "        # Returns\n",
    "            Output tensor for the block.\n",
    "        # Raises\n",
    "            ValueError: if `block_type` is not one of `'block35'`,\n",
    "                `'block17'` or `'block8'`.\n",
    "        \"\"\"\n",
    "        if block_type == 'block35':\n",
    "            branch_0 = InceptionResNet.conv2d_bn(x, 32, 1)\n",
    "            branch_1 = InceptionResNet.conv2d_bn(x, 32, 1)\n",
    "            branch_1 = InceptionResNet.conv2d_bn(branch_1, 32, 3)\n",
    "            branch_2 = InceptionResNet.conv2d_bn(x, 32, 1)\n",
    "            branch_2 = InceptionResNet.conv2d_bn(branch_2, 32, 3)\n",
    "            branch_2 = InceptionResNet.conv2d_bn(branch_2, 48, 3)\n",
    "            branches = [branch_0, branch_1, branch_2]\n",
    "        elif block_type == 'block17':\n",
    "            branch_0 = InceptionResNet.conv2d_bn(x, 192, 1)\n",
    "            branch_1 = InceptionResNet.conv2d_bn(x, 128, 1)\n",
    "            branch_1 = InceptionResNet.conv2d_bn(branch_1, 160, [1, 7])\n",
    "            branch_1 = InceptionResNet.conv2d_bn(branch_1, 192, [7, 1])\n",
    "            branches = [branch_0, branch_1]\n",
    "        elif block_type == 'block8':\n",
    "            branch_0 = InceptionResNet.conv2d_bn(x, 192, 1)\n",
    "            branch_1 = InceptionResNet.conv2d_bn(x, 192, 1)\n",
    "            branch_1 = InceptionResNet.conv2d_bn(branch_1, 224, [1, 3])\n",
    "            branch_1 = InceptionResNet.conv2d_bn(branch_1, 256, [3, 1])\n",
    "            branches = [branch_0, branch_1]\n",
    "        else:\n",
    "            raise ValueError('Unknown Inception-ResNet block type. '\n",
    "                             'Expects \"block35\", \"block17\" or \"block8\", '\n",
    "                             'but got: ' + str(block_type))\n",
    "\n",
    "        block_name = block_type + '_' + str(block_idx)\n",
    "        channel_axis = 1 if tf.keras.backend.image_data_format() == 'channels_first' else 3\n",
    "        mixed = tf.keras.layers.Concatenate(\n",
    "            axis=channel_axis, name=block_name + '_mixed')(branches)\n",
    "        up = InceptionResNet.conv2d_bn(mixed,\n",
    "                       tf.keras.backend.int_shape(x)[channel_axis],\n",
    "                       1,\n",
    "                       activation=None,\n",
    "                       use_bias=True,\n",
    "                       name=block_name + '_conv')\n",
    "\n",
    "        x = tf.keras.layers.Lambda(lambda inputs, scale: inputs[0] + inputs[1] * scale,\n",
    "                          output_shape=tf.keras.backend.int_shape(x)[1:],\n",
    "                          arguments={'scale': scale},\n",
    "                          name=block_name)([x, up])\n",
    "        if activation is not None:\n",
    "            x = tf.keras.layers.Activation(activation, name=block_name + '_ac')(x)\n",
    "        return x\n",
    "    \n",
    "    def get_model(self):\n",
    "        img_input = tf.keras.layers.Input(shape=self.input_shape)\n",
    "        # stem\n",
    "        x = InceptionResNet.conv2d_bn(img_input, 32, 3)\n",
    "        \n",
    "        # inception block No reduction 320\n",
    "        branch_0 = InceptionResNet.conv2d_bn(x, 32, 1)\n",
    "        branch_1 = InceptionResNet.conv2d_bn(x, 32, 1)\n",
    "        branch_1 = InceptionResNet.conv2d_bn(branch_1, 48, 5)\n",
    "        branch_2 = InceptionResNet.conv2d_bn(x, 32, 1)\n",
    "        branch_2 = InceptionResNet.conv2d_bn(branch_2, 32, 3)\n",
    "        branch_2 = InceptionResNet.conv2d_bn(branch_2, 48, 3)\n",
    "        branch_pool = tf.keras.layers.AveragePooling2D(3, strides=1, padding='same')(x)\n",
    "        branch_pool = InceptionResNet.conv2d_bn(branch_pool, 48, 1)\n",
    "        branches = [branch_0, branch_1, branch_2, branch_pool]\n",
    "        x = tf.keras.layers.Concatenate(axis=3, name='mixed_5b')(branches)\n",
    "        \n",
    "         # 3x block35 (Inception-ResNet-A block): No reduction, 320 channels\n",
    "        for block_idx in range(0, 2):\n",
    "            x = InceptionResNet.inception_resnet_block(x,\n",
    "                                       scale=0.17,\n",
    "                                       block_type='block35',\n",
    "                                       block_idx=block_idx)\n",
    "        \n",
    "        # Mixed 6a (Reduction-A block): half 1088\n",
    "        branch_0 = InceptionResNet.conv2d_bn(x, 32, 3, strides=2, padding='valid')\n",
    "        branch_1 = InceptionResNet.conv2d_bn(x, 32, 1)\n",
    "        branch_1 = InceptionResNet.conv2d_bn(branch_1, 32, 3)\n",
    "        branch_1 = InceptionResNet.conv2d_bn(branch_1, 48, 3, strides=2, padding='valid')\n",
    "        branch_pool = tf.keras.layers.MaxPooling2D(3, strides=2, padding='valid')(x)\n",
    "        branches = [branch_0, branch_1, branch_pool]\n",
    "        x = tf.keras.layers.Concatenate(axis=3, name='mixed_6a')(branches)\n",
    "        \n",
    "        \n",
    "        # 3x block17 (Inception-ResNet-B block): No reduction 1088\n",
    "        for block_idx in range(2, 4):\n",
    "            x = InceptionResNet.inception_resnet_block(x,\n",
    "                                       scale=0.1,\n",
    "                                       block_type='block35',\n",
    "                                       block_idx=block_idx)\n",
    "            \n",
    "        # Mixed 7a (Reduction-B block): half reduction x 2080\n",
    "        branch_0 = InceptionResNet.conv2d_bn(x, 32, 1)\n",
    "        branch_0 = InceptionResNet.conv2d_bn(branch_0, 32, 3, strides=2, padding='valid')\n",
    "        branch_1 = InceptionResNet.conv2d_bn(x, 32, 1)\n",
    "        branch_1 = InceptionResNet.conv2d_bn(branch_1, 48, 3, strides=2, padding='valid')\n",
    "        branch_2 = InceptionResNet.conv2d_bn(x, 32, 1)\n",
    "        branch_2 = InceptionResNet.conv2d_bn(branch_2, 32, 3)\n",
    "        branch_2 = InceptionResNet.conv2d_bn(branch_2, 48, 3, strides=2, padding='valid')\n",
    "        branch_pool = tf.keras.layers.MaxPooling2D(3, strides=2, padding='valid')(x)\n",
    "        branches = [branch_0, branch_1, branch_2, branch_pool]\n",
    "        x = tf.keras.layers.Concatenate(axis=3, name='mixed_7a')(branches)\n",
    "        \n",
    "        # (Inception-ResNet-C block): No Reduction 2080\n",
    "        for block_idx in range(4, 6):\n",
    "            x = InceptionResNet.inception_resnet_block(x,\n",
    "                                       scale=0.2,\n",
    "                                       block_type='block35',\n",
    "                                       block_idx=block_idx)\n",
    "            \n",
    "        x = InceptionResNet.inception_resnet_block(x,\n",
    "                               scale=1.,\n",
    "                               activation=None,\n",
    "                               block_type='block35',\n",
    "                               block_idx=6)\n",
    "\n",
    "        # Final convolution block: 512 channels\n",
    "        x = InceptionResNet.conv2d_bn(x, 512, 1, name='conv_7b')\n",
    "        \n",
    "        # Classification block\n",
    "        x = tf.keras.layers.GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "        x = tf.keras.layers.Dense(self.num_classes, activation='softmax', name='predictions')(x)\n",
    "        \n",
    "        return tf.keras.Model(img_input, x, name=\"InceptionResNet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = InceptionResNet(input_shape=(DATA_CROP_ROWS, DATA_CROP_COLS, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = builder.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"InceptionResNet\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 28, 28, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 28, 28, 32)   896         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 28, 28, 32)   96          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 28, 28, 32)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 28, 28, 32)   1056        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 28, 28, 32)   96          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 28, 28, 32)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 28, 28, 32)   1056        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 28, 28, 32)   9248        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 28, 28, 32)   96          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 28, 28, 32)   96          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 28, 28, 32)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 28, 28, 32)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 28, 28, 32)   0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 28, 28, 32)   1056        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 28, 28, 48)   38448       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 28, 28, 48)   13872       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 28, 28, 48)   1584        average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 28, 28, 32)   96          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 28, 28, 48)   144         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 28, 28, 48)   144         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 28, 28, 48)   144         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 28, 28, 32)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 28, 28, 48)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 28, 28, 48)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 28, 28, 48)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "mixed_5b (Concatenate)          (None, 28, 28, 176)  0           activation_1[0][0]               \n",
      "                                                                 activation_3[0][0]               \n",
      "                                                                 activation_6[0][0]               \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 28, 28, 32)   5664        mixed_5b[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 28, 28, 32)   96          conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 28, 28, 32)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 28, 28, 32)   5664        mixed_5b[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 28, 28, 32)   9248        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 28, 28, 32)   96          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 28, 28, 32)   96          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 28, 28, 32)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 28, 28, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 28, 28, 32)   5664        mixed_5b[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 28, 28, 32)   9248        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 28, 28, 48)   13872       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 28, 28, 32)   96          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 28, 28, 32)   96          conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 28, 28, 48)   144         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 28, 28, 32)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 28, 28, 32)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 28, 28, 48)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_0_mixed (Concatenate)   (None, 28, 28, 112)  0           activation_8[0][0]               \n",
      "                                                                 activation_10[0][0]              \n",
      "                                                                 activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block35_0_conv (Conv2D)         (None, 28, 28, 176)  19888       block35_0_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_0_conv_bn (BatchNormali (None, 28, 28, 176)  528         block35_0_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_0 (Lambda)              (None, 28, 28, 176)  0           mixed_5b[0][0]                   \n",
      "                                                                 block35_0_conv_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block35_0_ac (Activation)       (None, 28, 28, 176)  0           block35_0[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 28, 28, 32)   5664        block35_0_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 28, 28, 32)   96          conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 28, 28, 32)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 28, 28, 32)   5664        block35_0_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 28, 28, 32)   9248        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 28, 28, 32)   96          conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 28, 28, 32)   96          conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 28, 28, 32)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 28, 28, 32)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 28, 28, 32)   5664        block35_0_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 28, 28, 32)   9248        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 28, 28, 48)   13872       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 28, 28, 32)   96          conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 28, 28, 32)   96          conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 28, 28, 48)   144         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 28, 28, 32)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 28, 28, 32)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 28, 28, 48)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_1_mixed (Concatenate)   (None, 28, 28, 112)  0           activation_14[0][0]              \n",
      "                                                                 activation_16[0][0]              \n",
      "                                                                 activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block35_1_conv (Conv2D)         (None, 28, 28, 176)  19888       block35_1_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_1_conv_bn (BatchNormali (None, 28, 28, 176)  528         block35_1_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_1 (Lambda)              (None, 28, 28, 176)  0           block35_0_ac[0][0]               \n",
      "                                                                 block35_1_conv_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block35_1_ac (Activation)       (None, 28, 28, 176)  0           block35_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 28, 28, 32)   5664        block35_1_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 28, 28, 32)   96          conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 28, 28, 32)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 28, 28, 32)   9248        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 28, 28, 32)   96          conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 28, 28, 32)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 13, 13, 32)   50720       block35_1_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 13, 13, 48)   13872       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 13, 13, 32)   96          conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 13, 13, 48)   144         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 13, 13, 32)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 13, 13, 48)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 13, 13, 176)  0           block35_1_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "mixed_6a (Concatenate)          (None, 13, 13, 256)  0           activation_20[0][0]              \n",
      "                                                                 activation_23[0][0]              \n",
      "                                                                 max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 13, 13, 32)   8224        mixed_6a[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 13, 13, 32)   96          conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 13, 13, 32)   0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 13, 13, 32)   8224        mixed_6a[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 13, 13, 32)   9248        activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 13, 13, 32)   96          conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 13, 13, 32)   96          conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 13, 13, 32)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 13, 13, 32)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 13, 13, 32)   8224        mixed_6a[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 13, 13, 32)   9248        activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 13, 13, 48)   13872       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 13, 13, 32)   96          conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 13, 13, 32)   96          conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 13, 13, 48)   144         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 13, 13, 32)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 13, 13, 32)   0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 13, 13, 48)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_2_mixed (Concatenate)   (None, 13, 13, 112)  0           activation_24[0][0]              \n",
      "                                                                 activation_26[0][0]              \n",
      "                                                                 activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block35_2_conv (Conv2D)         (None, 13, 13, 256)  28928       block35_2_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_2_conv_bn (BatchNormali (None, 13, 13, 256)  768         block35_2_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_2 (Lambda)              (None, 13, 13, 256)  0           mixed_6a[0][0]                   \n",
      "                                                                 block35_2_conv_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block35_2_ac (Activation)       (None, 13, 13, 256)  0           block35_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 13, 13, 32)   8224        block35_2_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 13, 13, 32)   96          conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 13, 13, 32)   0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 13, 13, 32)   8224        block35_2_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 13, 13, 32)   9248        activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 13, 13, 32)   96          conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 13, 13, 32)   96          conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 13, 13, 32)   0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 13, 13, 32)   0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 13, 13, 32)   8224        block35_2_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 13, 13, 32)   9248        activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 13, 13, 48)   13872       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 13, 13, 32)   96          conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 13, 13, 32)   96          conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 13, 13, 48)   144         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 13, 13, 32)   0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 13, 13, 32)   0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 13, 13, 48)   0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_3_mixed (Concatenate)   (None, 13, 13, 112)  0           activation_30[0][0]              \n",
      "                                                                 activation_32[0][0]              \n",
      "                                                                 activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block35_3_conv (Conv2D)         (None, 13, 13, 256)  28928       block35_3_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_3_conv_bn (BatchNormali (None, 13, 13, 256)  768         block35_3_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_3 (Lambda)              (None, 13, 13, 256)  0           block35_2_ac[0][0]               \n",
      "                                                                 block35_3_conv_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block35_3_ac (Activation)       (None, 13, 13, 256)  0           block35_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 13, 13, 32)   8224        block35_3_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 13, 13, 32)   96          conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 13, 13, 32)   0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 13, 13, 32)   8224        block35_3_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 13, 13, 32)   8224        block35_3_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 13, 13, 32)   9248        activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 13, 13, 32)   96          conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 13, 13, 32)   96          conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 13, 13, 32)   96          conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 13, 13, 32)   0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 13, 13, 32)   0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 13, 13, 32)   0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 6, 6, 32)     9248        activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 6, 6, 48)     13872       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 6, 6, 48)     13872       activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 6, 6, 32)     96          conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 6, 6, 48)     144         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 6, 6, 48)     144         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 6, 6, 32)     0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 6, 6, 48)     0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 6, 6, 48)     0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 6, 6, 256)    0           block35_3_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "mixed_7a (Concatenate)          (None, 6, 6, 384)    0           activation_37[0][0]              \n",
      "                                                                 activation_39[0][0]              \n",
      "                                                                 activation_42[0][0]              \n",
      "                                                                 max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 6, 6, 32)     12320       mixed_7a[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 6, 6, 32)     96          conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 6, 6, 32)     0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 6, 6, 32)     12320       mixed_7a[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 6, 6, 32)     9248        activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 6, 6, 32)     96          conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 6, 6, 32)     96          conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 6, 6, 32)     0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 6, 6, 32)     0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 6, 6, 32)     12320       mixed_7a[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 6, 6, 32)     9248        activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 6, 6, 48)     13872       activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 6, 6, 32)     96          conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 6, 6, 32)     96          conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 6, 6, 48)     144         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 6, 6, 32)     0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 6, 6, 32)     0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 6, 6, 48)     0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_4_mixed (Concatenate)   (None, 6, 6, 112)    0           activation_43[0][0]              \n",
      "                                                                 activation_45[0][0]              \n",
      "                                                                 activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block35_4_conv (Conv2D)         (None, 6, 6, 384)    43392       block35_4_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_4_conv_bn (BatchNormali (None, 6, 6, 384)    1152        block35_4_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_4 (Lambda)              (None, 6, 6, 384)    0           mixed_7a[0][0]                   \n",
      "                                                                 block35_4_conv_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block35_4_ac (Activation)       (None, 6, 6, 384)    0           block35_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 6, 6, 32)     12320       block35_4_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 6, 6, 32)     96          conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 6, 6, 32)     0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 6, 6, 32)     12320       block35_4_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 6, 6, 32)     9248        activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 6, 6, 32)     96          conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 6, 6, 32)     96          conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 6, 6, 32)     0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 6, 6, 32)     0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 6, 6, 32)     12320       block35_4_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 6, 6, 32)     9248        activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 6, 6, 48)     13872       activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 6, 6, 32)     96          conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 6, 6, 32)     96          conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 6, 6, 48)     144         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 6, 6, 32)     0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 6, 6, 32)     0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 6, 6, 48)     0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_5_mixed (Concatenate)   (None, 6, 6, 112)    0           activation_49[0][0]              \n",
      "                                                                 activation_51[0][0]              \n",
      "                                                                 activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block35_5_conv (Conv2D)         (None, 6, 6, 384)    43392       block35_5_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_5_conv_bn (BatchNormali (None, 6, 6, 384)    1152        block35_5_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_5 (Lambda)              (None, 6, 6, 384)    0           block35_4_ac[0][0]               \n",
      "                                                                 block35_5_conv_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block35_5_ac (Activation)       (None, 6, 6, 384)    0           block35_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 6, 6, 32)     12320       block35_5_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 6, 6, 32)     96          conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 6, 6, 32)     0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 6, 6, 32)     12320       block35_5_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 6, 6, 32)     9248        activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 6, 6, 32)     96          conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 6, 6, 32)     96          conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 6, 6, 32)     0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 6, 6, 32)     0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 6, 6, 32)     12320       block35_5_ac[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 6, 6, 32)     9248        activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 6, 6, 48)     13872       activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 6, 6, 32)     96          conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 6, 6, 32)     96          conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 6, 6, 48)     144         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 6, 6, 32)     0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 6, 6, 32)     0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 6, 6, 48)     0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block35_6_mixed (Concatenate)   (None, 6, 6, 112)    0           activation_55[0][0]              \n",
      "                                                                 activation_57[0][0]              \n",
      "                                                                 activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block35_6_conv (Conv2D)         (None, 6, 6, 384)    43392       block35_6_mixed[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block35_6_conv_bn (BatchNormali (None, 6, 6, 384)    1152        block35_6_conv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block35_6 (Lambda)              (None, 6, 6, 384)    0           block35_5_ac[0][0]               \n",
      "                                                                 block35_6_conv_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv_7b (Conv2D)                (None, 6, 6, 512)    197120      block35_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_7b_bn (BatchNormalization) (None, 6, 6, 512)    1536        conv_7b[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_7b_ac (Activation)         (None, 6, 6, 512)    0           conv_7b_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 512)          0           conv_7b_ac[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Dense)             (None, 10)           5130        avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,082,538\n",
      "Trainable params: 1,073,162\n",
      "Non-trainable params: 9,376\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./tb_logdir\", write_graph=True, profile_batch=0)\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(patience=10, verbose=1, factor=0.5)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=\"cifar_full_model_with_architecture.h5\", monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=100, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "   1563/Unknown - 75s 48ms/step - loss: 1.0860 - accuracy: 0.6133\n",
      "Epoch 00001: val_loss improved from inf to 1.13299, saving model to cifar_full_model_with_architecture.h5\n",
      "1563/1563 [==============================] - 81s 52ms/step - loss: 1.0860 - accuracy: 0.6133 - val_loss: 1.1330 - val_accuracy: 0.6250\n",
      "Epoch 2/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.7128 - accuracy: 0.7530\n",
      "Epoch 00002: val_loss improved from 1.13299 to 0.79415, saving model to cifar_full_model_with_architecture.h5\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.7128 - accuracy: 0.7530 - val_loss: 0.7941 - val_accuracy: 0.7301\n",
      "Epoch 3/300\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.5933 - accuracy: 0.7950\n",
      "Epoch 00003: val_loss improved from 0.79415 to 0.61627, saving model to cifar_full_model_with_architecture.h5\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.5930 - accuracy: 0.7950 - val_loss: 0.6163 - val_accuracy: 0.7882\n",
      "Epoch 4/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.5148 - accuracy: 0.8221\n",
      "Epoch 00004: val_loss did not improve from 0.61627\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 0.5153 - accuracy: 0.8220 - val_loss: 0.6838 - val_accuracy: 0.7720\n",
      "Epoch 5/300\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.4563 - accuracy: 0.8427\n",
      "Epoch 00005: val_loss improved from 0.61627 to 0.55898, saving model to cifar_full_model_with_architecture.h5\n",
      "1563/1563 [==============================] - 71s 45ms/step - loss: 0.4562 - accuracy: 0.8427 - val_loss: 0.5590 - val_accuracy: 0.8129\n",
      "Epoch 6/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.4133 - accuracy: 0.8582\n",
      "Epoch 00006: val_loss improved from 0.55898 to 0.50321, saving model to cifar_full_model_with_architecture.h5\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.4135 - accuracy: 0.8582 - val_loss: 0.5032 - val_accuracy: 0.8339\n",
      "Epoch 7/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.3805 - accuracy: 0.8679\n",
      "Epoch 00007: val_loss did not improve from 0.50321\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.3811 - accuracy: 0.8677 - val_loss: 0.5604 - val_accuracy: 0.8188\n",
      "Epoch 8/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.3494 - accuracy: 0.8798\n",
      "Epoch 00008: val_loss did not improve from 0.50321\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.3494 - accuracy: 0.8797 - val_loss: 0.5247 - val_accuracy: 0.8283\n",
      "Epoch 9/300\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.3247 - accuracy: 0.8882\n",
      "Epoch 00009: val_loss improved from 0.50321 to 0.44336, saving model to cifar_full_model_with_architecture.h5\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.3247 - accuracy: 0.8882 - val_loss: 0.4434 - val_accuracy: 0.8507\n",
      "Epoch 10/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.3045 - accuracy: 0.8940\n",
      "Epoch 00010: val_loss improved from 0.44336 to 0.40791, saving model to cifar_full_model_with_architecture.h5\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.3045 - accuracy: 0.8940 - val_loss: 0.4079 - val_accuracy: 0.8655\n",
      "Epoch 11/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.2842 - accuracy: 0.9015\n",
      "Epoch 00011: val_loss improved from 0.40791 to 0.39611, saving model to cifar_full_model_with_architecture.h5\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.2843 - accuracy: 0.9014 - val_loss: 0.3961 - val_accuracy: 0.8728\n",
      "Epoch 12/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.2681 - accuracy: 0.9075\n",
      "Epoch 00012: val_loss improved from 0.39611 to 0.37346, saving model to cifar_full_model_with_architecture.h5\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.2681 - accuracy: 0.9074 - val_loss: 0.3735 - val_accuracy: 0.8823\n",
      "Epoch 13/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.2498 - accuracy: 0.9143\n",
      "Epoch 00013: val_loss did not improve from 0.37346\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.2497 - accuracy: 0.9143 - val_loss: 0.3930 - val_accuracy: 0.8746\n",
      "Epoch 14/300\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.2383 - accuracy: 0.9180\n",
      "Epoch 00014: val_loss did not improve from 0.37346\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.2382 - accuracy: 0.9180 - val_loss: 0.4201 - val_accuracy: 0.8725\n",
      "Epoch 15/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.2252 - accuracy: 0.9216\n",
      "Epoch 00015: val_loss improved from 0.37346 to 0.36157, saving model to cifar_full_model_with_architecture.h5\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.2253 - accuracy: 0.9216 - val_loss: 0.3616 - val_accuracy: 0.8790\n",
      "Epoch 16/300\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.2140 - accuracy: 0.9257\n",
      "Epoch 00016: val_loss did not improve from 0.36157\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.2141 - accuracy: 0.9256 - val_loss: 0.3726 - val_accuracy: 0.8853\n",
      "Epoch 17/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.2074 - accuracy: 0.9282\n",
      "Epoch 00017: val_loss did not improve from 0.36157\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.2072 - accuracy: 0.9283 - val_loss: 0.4046 - val_accuracy: 0.8700\n",
      "Epoch 18/300\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.1931 - accuracy: 0.9324\n",
      "Epoch 00018: val_loss did not improve from 0.36157\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.1931 - accuracy: 0.9324 - val_loss: 0.3671 - val_accuracy: 0.8833\n",
      "Epoch 19/300\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.1835 - accuracy: 0.9362\n",
      "Epoch 00019: val_loss did not improve from 0.36157\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.1834 - accuracy: 0.9362 - val_loss: 0.3774 - val_accuracy: 0.8806\n",
      "Epoch 20/300\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.1768 - accuracy: 0.9376\n",
      "Epoch 00020: val_loss improved from 0.36157 to 0.36147, saving model to cifar_full_model_with_architecture.h5\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.1769 - accuracy: 0.9376 - val_loss: 0.3615 - val_accuracy: 0.8893\n",
      "Epoch 21/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.1675 - accuracy: 0.9412\n",
      "Epoch 00021: val_loss did not improve from 0.36147\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.1678 - accuracy: 0.9412 - val_loss: 0.3908 - val_accuracy: 0.8813\n",
      "Epoch 22/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.1583 - accuracy: 0.9444\n",
      "Epoch 00022: val_loss did not improve from 0.36147\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.1582 - accuracy: 0.9444 - val_loss: 0.4028 - val_accuracy: 0.8765\n",
      "Epoch 23/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.1549 - accuracy: 0.9455\n",
      "Epoch 00023: val_loss improved from 0.36147 to 0.33559, saving model to cifar_full_model_with_architecture.h5\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.1549 - accuracy: 0.9455 - val_loss: 0.3356 - val_accuracy: 0.8944\n",
      "Epoch 24/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.1468 - accuracy: 0.9486\n",
      "Epoch 00024: val_loss did not improve from 0.33559\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.1469 - accuracy: 0.9485 - val_loss: 0.3667 - val_accuracy: 0.8914\n",
      "Epoch 25/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.1395 - accuracy: 0.9508\n",
      "Epoch 00025: val_loss did not improve from 0.33559\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.1396 - accuracy: 0.9507 - val_loss: 0.3846 - val_accuracy: 0.8891\n",
      "Epoch 26/300\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.1384 - accuracy: 0.9506\n",
      "Epoch 00026: val_loss did not improve from 0.33559\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.1386 - accuracy: 0.9506 - val_loss: 0.3739 - val_accuracy: 0.8895\n",
      "Epoch 27/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.1341 - accuracy: 0.9515\n",
      "Epoch 00027: val_loss did not improve from 0.33559\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.1341 - accuracy: 0.9515 - val_loss: 0.3883 - val_accuracy: 0.8895\n",
      "Epoch 28/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.1273 - accuracy: 0.9542\n",
      "Epoch 00028: val_loss did not improve from 0.33559\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.1273 - accuracy: 0.9542 - val_loss: 0.3422 - val_accuracy: 0.8992\n",
      "Epoch 29/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.1191 - accuracy: 0.9577\n",
      "Epoch 00029: val_loss did not improve from 0.33559\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.1191 - accuracy: 0.9577 - val_loss: 0.4687 - val_accuracy: 0.8743\n",
      "Epoch 30/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.1224 - accuracy: 0.9572\n",
      "Epoch 00030: val_loss did not improve from 0.33559\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.1228 - accuracy: 0.9571 - val_loss: 0.3873 - val_accuracy: 0.8908\n",
      "Epoch 31/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.1125 - accuracy: 0.9602\n",
      "Epoch 00031: val_loss did not improve from 0.33559\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.1125 - accuracy: 0.9602 - val_loss: 0.4568 - val_accuracy: 0.8766\n",
      "Epoch 32/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.1127 - accuracy: 0.9604\n",
      "Epoch 00032: val_loss did not improve from 0.33559\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.1126 - accuracy: 0.9604 - val_loss: 0.3700 - val_accuracy: 0.8983\n",
      "Epoch 33/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.1061 - accuracy: 0.9629\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.33559\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.1060 - accuracy: 0.9629 - val_loss: 0.3776 - val_accuracy: 0.8965\n",
      "Epoch 34/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.0685 - accuracy: 0.9768\n",
      "Epoch 00034: val_loss improved from 0.33559 to 0.29943, saving model to cifar_full_model_with_architecture.h5\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.0685 - accuracy: 0.9768 - val_loss: 0.2994 - val_accuracy: 0.9148\n",
      "Epoch 35/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.0616 - accuracy: 0.9790\n",
      "Epoch 00035: val_loss did not improve from 0.29943\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.0617 - accuracy: 0.9790 - val_loss: 0.3110 - val_accuracy: 0.9142\n",
      "Epoch 36/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.0574 - accuracy: 0.9804\n",
      "Epoch 00036: val_loss did not improve from 0.29943\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.0574 - accuracy: 0.9804 - val_loss: 0.3323 - val_accuracy: 0.9135\n",
      "Epoch 37/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.0539 - accuracy: 0.9811\n",
      "Epoch 00037: val_loss did not improve from 0.29943\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.0540 - accuracy: 0.9811 - val_loss: 0.3396 - val_accuracy: 0.9136\n",
      "Epoch 38/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.0525 - accuracy: 0.9814\n",
      "Epoch 00038: val_loss did not improve from 0.29943\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.0525 - accuracy: 0.9814 - val_loss: 0.3511 - val_accuracy: 0.9096\n",
      "Epoch 39/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.0496 - accuracy: 0.9828\n",
      "Epoch 00039: val_loss did not improve from 0.29943\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.0497 - accuracy: 0.9828 - val_loss: 0.3920 - val_accuracy: 0.9015\n",
      "Epoch 40/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.0482 - accuracy: 0.9837\n",
      "Epoch 00040: val_loss did not improve from 0.29943\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.0482 - accuracy: 0.9837 - val_loss: 0.3481 - val_accuracy: 0.9134\n",
      "Epoch 41/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.0471 - accuracy: 0.9836\n",
      "Epoch 00041: val_loss did not improve from 0.29943\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.0471 - accuracy: 0.9836 - val_loss: 0.3531 - val_accuracy: 0.9126\n",
      "Epoch 42/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.0438 - accuracy: 0.9851\n",
      "Epoch 00042: val_loss did not improve from 0.29943\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.0438 - accuracy: 0.9851 - val_loss: 0.3704 - val_accuracy: 0.9089\n",
      "Epoch 43/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.0449 - accuracy: 0.9842\n",
      "Epoch 00043: val_loss did not improve from 0.29943\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.0449 - accuracy: 0.9842 - val_loss: 0.3772 - val_accuracy: 0.9079\n",
      "Epoch 44/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.0440 - accuracy: 0.9844\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.29943\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.0439 - accuracy: 0.9844 - val_loss: 0.3594 - val_accuracy: 0.9128\n",
      "Epoch 45/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.0314 - accuracy: 0.9895\n",
      "Epoch 00045: val_loss did not improve from 0.29943\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.0314 - accuracy: 0.9895 - val_loss: 0.3328 - val_accuracy: 0.9167\n",
      "Epoch 46/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.0261 - accuracy: 0.9912\n",
      "Epoch 00046: val_loss did not improve from 0.29943\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.0261 - accuracy: 0.9911 - val_loss: 0.3338 - val_accuracy: 0.9188\n",
      "Epoch 47/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.0264 - accuracy: 0.9910\n",
      "Epoch 00047: val_loss did not improve from 0.29943\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.0264 - accuracy: 0.9910 - val_loss: 0.3384 - val_accuracy: 0.9196\n",
      "Epoch 48/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.0230 - accuracy: 0.9921\n",
      "Epoch 00048: val_loss did not improve from 0.29943\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.0230 - accuracy: 0.9921 - val_loss: 0.3552 - val_accuracy: 0.9185\n",
      "Epoch 49/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.0234 - accuracy: 0.9919\n",
      "Epoch 00049: val_loss did not improve from 0.29943\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.0234 - accuracy: 0.9919 - val_loss: 0.3538 - val_accuracy: 0.9197\n",
      "Epoch 50/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.0232 - accuracy: 0.9921\n",
      "Epoch 00050: val_loss did not improve from 0.29943\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.0232 - accuracy: 0.9921 - val_loss: 0.3494 - val_accuracy: 0.9180\n",
      "Epoch 51/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.0193 - accuracy: 0.9939\n",
      "Epoch 00051: val_loss did not improve from 0.29943\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.0193 - accuracy: 0.9939 - val_loss: 0.3523 - val_accuracy: 0.9174\n",
      "Epoch 52/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.0213 - accuracy: 0.9929\n",
      "Epoch 00052: val_loss did not improve from 0.29943\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.0213 - accuracy: 0.9929 - val_loss: 0.3601 - val_accuracy: 0.9191\n",
      "Epoch 53/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.0208 - accuracy: 0.9931\n",
      "Epoch 00053: val_loss did not improve from 0.29943\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.0208 - accuracy: 0.9931 - val_loss: 0.3654 - val_accuracy: 0.9164\n",
      "Epoch 54/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.0192 - accuracy: 0.9937\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.29943\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.0192 - accuracy: 0.9937 - val_loss: 0.3571 - val_accuracy: 0.9153\n",
      "Epoch 55/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.0157 - accuracy: 0.9948\n",
      "Epoch 00055: val_loss did not improve from 0.29943\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.0157 - accuracy: 0.9948 - val_loss: 0.3425 - val_accuracy: 0.9196\n",
      "Epoch 56/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.0136 - accuracy: 0.9956\n",
      "Epoch 00056: val_loss did not improve from 0.29943\n",
      "1563/1563 [==============================] - 68s 43ms/step - loss: 0.0135 - accuracy: 0.9956 - val_loss: 0.3479 - val_accuracy: 0.9208\n",
      "Epoch 57/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.0133 - accuracy: 0.9956\n",
      "Epoch 00057: val_loss did not improve from 0.29943\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.0133 - accuracy: 0.9956 - val_loss: 0.3548 - val_accuracy: 0.9219\n",
      "Epoch 58/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.0139 - accuracy: 0.9955\n",
      "Epoch 00058: val_loss did not improve from 0.29943\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.0139 - accuracy: 0.9955 - val_loss: 0.3511 - val_accuracy: 0.9210\n",
      "Epoch 59/300\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.0126 - accuracy: 0.9960\n",
      "Epoch 00059: val_loss did not improve from 0.29943\n",
      "1563/1563 [==============================] - 67s 43ms/step - loss: 0.0126 - accuracy: 0.9960 - val_loss: 0.3586 - val_accuracy: 0.9206\n",
      "Epoch 00059: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=dataset_train, epochs=EPOCHS, verbose=1, validation_data=dataset_test, callbacks=[tb_callback, lr_scheduler, checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_training_curves(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
